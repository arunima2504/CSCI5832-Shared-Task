{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfNVqs8V427q",
        "outputId": "aa4ad794-bcbe-4e5d-9bcb-55f1ba9c1316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CSCI5832-Shared-Task'...\n",
            "remote: Enumerating objects: 1060, done.\u001b[K\n",
            "remote: Counting objects: 100% (1060/1060), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1052/1052), done.\u001b[K\n",
            "remote: Total 1060 (delta 15), reused 1037 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1060/1060), 3.29 MiB | 12.08 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/CSCI5832-team-30/CSCI5832-Shared-Task.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puJI7ZH240FU",
        "outputId": "a88dcee8-a914-43a7-e233-aeb6279b6f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Load data\n",
        "train_path = \"/content/CSCI5832-Shared-Task/training_data/train.json\"\n",
        "with open(train_path) as json_file:\n",
        "    train_data = json.load(json_file)\n",
        "\n",
        "#Load dev set\n",
        "dev_path = \"/content/CSCI5832-Shared-Task/training_data/dev.json\"\n",
        "with open(dev_path) as json_file:\n",
        "    val_data = json.load(json_file)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_data, test_data = train_test_split(list(train_data.values()), test_size=0.2, random_state=42)\n",
        "\n",
        "val_data= list(val_data.values())\n",
        "\n",
        "# Define a custom dataset class\n",
        "class ClinicalTrialsDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        statement = item[\"Statement\"]\n",
        "        label = 1 if item[\"Label\"] == \"Entailment\" else 0  # Convert labels to binary (0 or 1)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            statement,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 8\n",
        "num_epochs = 8\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = ClinicalTrialsDataset(train_data, tokenizer)\n",
        "test_dataset = ClinicalTrialsDataset(test_data, tokenizer)\n",
        "val_dataset = ClinicalTrialsDataset(val_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"label\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Record predictions and labels for training evaluation\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        all_train_preds.extend(preds.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate and print training F1 score\n",
        "    train_f_score = f1_score(all_train_labels, all_train_preds)\n",
        "    print(f\"Epoch {epoch + 1} - Training F1 Score: {train_f_score:.4f}\")\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "all_test_preds = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"label\"]\n",
        "\n",
        "        outputs = model(inputs, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_test_preds.extend(preds.cpu().numpy())\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print test set F1 score\n",
        "test_f_score = f1_score(all_test_labels, all_test_preds)\n",
        "print(f\"Test Set F1 Score: {test_f_score:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"label\"]\n",
        "\n",
        "        outputs = model(inputs, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "f_score = f1_score(all_labels, all_preds)\n",
        "p_score = precision_score(all_labels, all_preds)\n",
        "r_score = recall_score(all_labels, all_preds)\n",
        "\n",
        "print('F1:{:f}'.format(f_score))\n",
        "print('precision_score:{:f}'.format(p_score))\n",
        "print('recall_score:{:f}'.format(r_score))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fGEelUKke7t",
        "outputId": "c33050f4-aa5c-428a-b0ca-656b5d25e20a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training F1 Score: 0.5459\n",
            "Epoch 2 - Training F1 Score: 0.4942\n",
            "Epoch 3 - Training F1 Score: 0.5122\n",
            "Epoch 4 - Training F1 Score: 0.5405\n",
            "Epoch 5 - Training F1 Score: 0.5131\n",
            "Epoch 6 - Training F1 Score: 0.5361\n",
            "Epoch 7 - Training F1 Score: 0.5904\n",
            "Epoch 8 - Training F1 Score: 0.5695\n",
            "Test Set F1 Score: 0.5504\n",
            "F1:0.598131\n",
            "precision_score:0.561404\n",
            "recall_score:0.640000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}